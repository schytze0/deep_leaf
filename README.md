# ğŸŒ± Deep Leaf - Plant Disease Classification MLOps Pipeline

## ğŸ“Œ Overview
**Deep Leaf** is a deep learning-based **image classification pipeline** for detecting plant diseases using **Transfer Learning (VGG16)**. It follows **MLOps best practices**, enabling:
- **FastAPI access**
- **MLflow tracking**
- **Airflow orchestration**
- **CI/CD with Github**

## Context??
DO WE NEED THIS?

## ğŸ“‚ Repository Structure

We saved all necessary files for the python runs into the folder `src/`. To use these scripts in the FastAPI (folder `app/`), we create a local package of `src` that is built during installing the requirements (`requirements.txt` or `requirements_mac.txt`). 
The `data/` is once build with the script `raw_data_split.py` and then saved into `data/raw`. Since we simulate new data by adding to the first data set the each of the other data splits (up to 10) we create two new files `train.tfrecord` and `valid.tfrecord` that are saved in `data/training/`.
In `model/`, you can find the current production model (`production_model.keras`) as well as metadata (final validation accuracy score, `metadata.txt`).
In `app/`, you find the creation of the FastAPI.
In `mlflow/`, you can find the creation of the MLflow container. 
In `tests/`, you can find simple test scripts for the unit tests.
There are some helper files:
- `setup.py`: for creation of the package `src` to reference them in `app/`
- `architecture.excalidraw`: visualization of (ongoing) workflow
- `merge_progress.json`: A file to check how far we have been so far with the new data simulation 

```plaintext
.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ app
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ architecture.excalidraw.png
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ raw
â”‚   â”‚   â”œâ”€â”€ train_subset1.tfrecord
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ train_subset10.tfrecord
â”‚   â”‚   â”œâ”€â”€ valid_subset1.tfrecord
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ valid_subset10.tfrecord
â”‚   â”œâ”€â”€ test
â”‚   â”‚   â”œâ”€â”€ AppleCedarRust1.JPG
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ AppleCedarRust4.JPG
â”‚   â”‚   â”œâ”€â”€ AppleScab1.JPG
â”‚   â”‚   â”œâ”€â”€ AppleScab2.JPG
â”‚   â”‚   â”œâ”€â”€ AppleScab3.JPG
â”‚   â”‚   â”œâ”€â”€ CornCommonRust1.JPG
â”‚   â”‚   â”œâ”€â”€ CornCommonRust2.JPG
â”‚   â”‚   â”œâ”€â”€ CornCommonRust3.JPG
â”‚   â”‚   â”œâ”€â”€ PotatoEarlyBlight1.JPG
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ PotatoEarlyBlight5.JPG
â”‚   â”‚   â”œâ”€â”€ PotatoHealthy1.JPG
â”‚   â”‚   â”œâ”€â”€ PotatoHealthy2.JPG
â”‚   â”‚   â”œâ”€â”€ TomatoEarlyBlight1.JPG
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ TomatoEarlyBlight6.JPG
â”‚   â”‚   â”œâ”€â”€ TomatoHealthy1.JPG
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ TomatoHealthy4.JPG
â”‚   â”‚   â”œâ”€â”€ TomatoYellowCurlVirus1.JPG
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ TomatoYellowCurlVirus6.JPG
â”‚   â””â”€â”€ training
â”‚       â”œâ”€â”€ train.tfrecord
â”‚       â””â”€â”€ valid.tfrecord
â”œâ”€â”€ data.dvc
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ dockers
â”‚   â”œâ”€â”€ airflow
â”‚   â”‚   â”œâ”€â”€ dags
â”‚   â”‚   â”‚   â””â”€â”€ dag_train.py
â”‚   â”‚   â”œâ”€â”€ logs
â”‚   â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”‚   â”œâ”€â”€ dag_processor_manager
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ dag_processor_manager.log
â”‚   â”‚   â”‚   â””â”€â”€ scheduler
â”‚   â”‚   â”‚       â”œâ”€â”€ 2025-03-20
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ dag_train.py.log
â”‚   â”‚   â”‚       â””â”€â”€ latest -> 2025-03-20
â”‚   â”‚   â””â”€â”€ plugins
â”‚   â”œâ”€â”€ fastapi
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â””â”€â”€ mlflow
â”‚       â””â”€â”€ Dockerfile
â”œâ”€â”€ logs
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ history_20250319_002320.json
â”œâ”€â”€ merge_progress.json
â”œâ”€â”€ mlflow
â”‚   â””â”€â”€ artifacts
â”‚       â””â”€â”€ ...
â”œâ”€â”€ mlflow.dvc
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ metadata.txt
â”‚   â”œâ”€â”€ production_model.keras
â”‚   â””â”€â”€ production_model.keras.dvc
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements_mac.txt
â”œâ”€â”€ requirements_wsl2.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ local_dagshub
â”‚   â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”‚   â”œâ”€â”€ prod_model_select_mlflow_dagshub.py
â”‚   â”‚   â””â”€â”€ train_mlflow_dagshub.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ git_dvc_update.py
â”‚   â”œâ”€â”€ helpers.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ predict.py
â”‚   â”œâ”€â”€ prod_model_select.py
â”‚   â”œâ”€â”€ raw_data_split.py
â”‚   â”œâ”€â”€ test_config.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ trials.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ temp
â”‚   â”œâ”€â”€ current_accuracy.txt
â”‚   â””â”€â”€ current_model.keras
â””â”€â”€ tests
    â”œâ”€â”€ api_server.py
    â””â”€â”€ mlflow_server.py
```

## ğŸ“ˆ Data
The original data stems from [Kaggle (New Plant Diseases Dataset)](https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset). Before the project, we downloaded the data set once, and created 10 subsets of training and validation (script `src/raw_data_split.py`), saving the subsets as `.tfrecord` for versioning and then used this 10 subsets as fictional new data income. Therefore, we will add incrementally to the first data set, the other splits to simulate new data incoming. 

## ğŸ§‘â€ğŸ’» Project Diagram

![Projekt worklfow implementation](architecture.excalidraw.png)

## Application Operation

### FastAPI

*Description*

## MLflow
This project integrates MLflow to track model training, log hyperparameters, and store artifacts for versioning and reproducibility.

1. Tracking and Logging

During training, MLflow logs:
	â€¢	Metrics: Training loss, accuracy, F1-score, validation loss, validation accuracy, and validation F1-score for each epoch.
	â€¢	Hyperparameters: Model type (VGG16), number of epochs, batch size, input shape, and number of classes.
	â€¢	Artifacts: The trained model is saved using mlflow.keras.log_model() for reproducibility.

2. Experiment Setup

Before training starts, MLflow:
	â€¢	Sets the tracking URI from environment variables (MLFLOW_TRACKING_URI).
	â€¢	Defines an experiment name (MLFLOW_EXPERIMENT_NAME).
	â€¢	Initializes default logging for hyperparameters.

3. Logging During Training

A custom callback (MLFlowLogger) logs training metrics at the end of each epoch:
	â€¢	Tracks best validation accuracy and best F1-score across epochs.
	â€¢	Logs final validation metrics after training.

4. Model Storage & Comparison
	â€¢	The trained model is saved in MLflowâ€™s model registry.
	â€¢	The current model is stored locally (temp/current_model.keras) for comparison with previous models.

5. Reproducibility

The training history (accuracy, F1-score, loss) is saved as a JSON file, ensuring results can be analyzed later.

MLflow is set up in a container running the tracking server. We use PostgreSQL database (mlflow-postgres) as backend for tracking experiment metadata. The container stores artifacts in `/app/mflow/artifacts/` which is mounted from the host machine. Access is given via port `5001`.

## Airflow

*Description*

