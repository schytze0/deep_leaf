# ğŸŒ± Deep Leaf - Plant Disease Classification MLOps Pipeline

## ğŸ“Œ Overview
**Deep Leaf** is a deep learning-based **image classification pipeline** for detecting plant diseases using **Transfer Learning (VGG16)**. It follows **MLOps best practices**, enabling:
- **FastAPI access**
- **MLflow tracking**
- **Airflow orchestration**
- **CI/CD with Github**

## Context??
DO WE NEED THIS?

## ğŸ“‚ Repository Structure

We saved all necessary files for the python runs into the folder `src/`. To use these scripts in the FastAPI (folder `app/`), we create a local package of `src` that is built during installing the requirements (`requirements.txt` or `requirements_mac.txt`). 
The `data/` is once build with the script `raw_data_split.py` and then saved into `data/raw`. Since we simulate new data by adding to the first data set the each of the other data splits (up to 10) we create two new files `train.tfrecord` and `valid.tfrecord` that are saved in `data/training/`.
In `model/`, you can find the current production model (`production_model.keras`) as well as metadata (final validation accuracy score, `metadata.txt`).
In `app/`, you find the creation of the FastAPI.
In `mlflow/`, you can find the creation of the MLflow container. 
In `tests/`, you can find simple test scripts for the unit tests.
There are some helper files:
- `setup.py`: for creation of the package `src` to reference them in `app/`
- `architecture.excalidraw`: visualization of (ongoing) workflow
- `merge_progress.json`: A file to check how far we have been so far with the new data simulation 

```plaintext
.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ app
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ __init__.cpython-311.pyc
â”‚   â”‚   â”œâ”€â”€ __init__.cpython-38.pyc
â”‚   â”‚   â”œâ”€â”€ main.cpython-311.pyc
â”‚   â”‚   â””â”€â”€ main.cpython-38.pyc
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ architecture.excalidraw.png
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ raw
â”‚   â”‚   â”œâ”€â”€ train_subset1.tfrecord
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ train_subset10.tfrecord
â”‚   â”‚   â”œâ”€â”€ valid_subset1.tfrecord
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ valid_subset10.tfrecord
â”‚   â”œâ”€â”€ test
â”‚   â”‚   â”œâ”€â”€ AppleCedarRust1.JPG
â”‚   â”‚   â”œâ”€â”€ ..
â”‚   â”‚   â”œâ”€â”€ AppleCedarRust4.JPG
â”‚   â”‚   â”œâ”€â”€ AppleScab1.JPG
â”‚   â”‚   â”œâ”€â”€ AppleScab2.JPG
â”‚   â”‚   â”œâ”€â”€ AppleScab3.JPG
â”‚   â”‚   â”œâ”€â”€ CornCommonRust1.JPG
â”‚   â”‚   â”œâ”€â”€ CornCommonRust2.JPG
â”‚   â”‚   â”œâ”€â”€ CornCommonRust3.JPG
â”‚   â”‚   â”œâ”€â”€ PotatoEarlyBlight1.JPG
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ PotatoEarlyBlight5.JPG
â”‚   â”‚   â”œâ”€â”€ PotatoHealthy1.JPG
â”‚   â”‚   â”œâ”€â”€ PotatoHealthy2.JPG
â”‚   â”‚   â”œâ”€â”€ TomatoEarlyBlight1.JPG
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ TomatoEarlyBlight6.JPG
â”‚   â”‚   â”œâ”€â”€ TomatoHealthy1.JPG
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ TomatoHealthy4.JPG
â”‚   â”‚   â”œâ”€â”€ TomatoYellowCurlVirus1.JPG
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ TomatoYellowCurlVirus6.JPG
â”‚   â””â”€â”€ training
â”‚       â”œâ”€â”€ train.tfrecord
â”‚       â””â”€â”€ valid.tfrecord
â”œâ”€â”€ data.dvc
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ dockers
â”‚   â”œâ”€â”€ airflow
â”‚   â”‚   â”œâ”€â”€ dags
â”‚   â”‚   â”œâ”€â”€ logs
â”‚   â”‚   â”‚   â”œâ”€â”€ dag_processor_manager
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ dag_processor_manager.log
â”‚   â”‚   â”‚   â””â”€â”€ scheduler
â”‚   â”‚   â”‚       â”œâ”€â”€ 2025-03-18
â”‚   â”‚   â”‚       â””â”€â”€ latest -> 2025-03-18
â”‚   â”‚   â””â”€â”€ plugins
â”‚   â”œâ”€â”€ fastapi
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â””â”€â”€ mlflow
â”‚       â””â”€â”€ Dockerfile
â”œâ”€â”€ logs
â”‚   â”œâ”€â”€ history_20250213_084609.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ merge_progress.json
â”œâ”€â”€ mlflow
â”‚   â””â”€â”€ artifacts
â”‚       â””â”€â”€ ...
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ metadata.txt
â”‚   â””â”€â”€ production_model.keras
â”œâ”€â”€ production_model.keras.dvc
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements_mac.txt
â”œâ”€â”€ requirements_wsl2.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ helpers.py
â”‚   â”œâ”€â”€ local_dagshub
â”‚   â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”‚   â”œâ”€â”€ prod_model_select_mlflow_dagshub.py
â”‚   â”‚   â””â”€â”€ train_mlflow_dagshub.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ predict.py
â”‚   â”œâ”€â”€ prod_model_select.py
â”‚   â”œâ”€â”€ prod_model_select_erwin.py
â”‚   â”œâ”€â”€ raw_data_split.py
â”‚   â”œâ”€â”€ test_config.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ train_erwin.py
â”‚   â”œâ”€â”€ trials.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ temp
â”‚   â”œâ”€â”€ current_accuracy.txt
â”‚   â””â”€â”€ current_model.keras
â””â”€â”€ tests
    â”œâ”€â”€ api_server.py
    â””â”€â”€ mlflow_server.py
```

## ğŸ“ˆ Data
The original data stems from [Kaggle (New Plant Diseases Dataset)](https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset). Before the project, we downloaded the data set once, and created 10 subsets of training and validation (script `src/raw_data_split.py`), saving the subsets as `.tfrecord` for versioning and then used this 10 subsets as fictional new data income. Therefore, we will add incrementally to the first data set, the other splits to simulate new data incoming. 

## ğŸ§‘â€ğŸ’» Project Diagram

![Projekt worklfow implementation](architecture.excalidraw.png)

## Application Operation

**FastAPI:**

*Description*

**MLflow:**
We created an independent container to run MLflow. This container saves artifacts and metrics and is linked to local volumes so that data is not lost after shutting down the container. 

**Airflow :**

*Description*

